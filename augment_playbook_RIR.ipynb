{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a30ce8",
   "metadata": {},
   "source": [
    "## Far-Field Audio Synthesis Toolkit (FAST) v0.2: User Guide\n",
    "This notebook serves as a user guide for the FAST toolkit, designed to rapidly synthesis audio to train/fine-tune audio ML models.\n",
    "\n",
    "#### ***v0.2 enhancements\n",
    "- Developed generation parameters logging and data regeneration function\n",
    "- Fixed bugs causing errors for some generation scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964f44",
   "metadata": {},
   "source": [
    "### Table of Content:\n",
    "- I. [Overview](#i-overview)\n",
    "- II. [Getting Started](#ii-overview)\n",
    "- III. [Synthesis Walkthrough](#iii-synthesis-walkthrough)\n",
    "- IV. [Bulk Audio Generation](#iv-bulk-audio-generation)\n",
    "- V. [Experimental Results (WIP)](#v-experimental-results-wip)\n",
    "- VI. [Regenerate Dataset](#vi-regeneration-of-datasets)\n",
    "- VII. [Contact Us](#vi-contact-us)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aa10b",
   "metadata": {},
   "source": [
    "### I. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa35987",
   "metadata": {},
   "source": [
    "#### I-A: Why FAST?\n",
    "- To augment training data outside datasets used for conventional commerical/open-research\n",
    "- Particularly, shortage of noisy labelled noisy data\n",
    "\n",
    "#### II-B: Synthesis Approach\n",
    "- WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f22087",
   "metadata": {},
   "source": [
    "### II. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f73f3",
   "metadata": {},
   "source": [
    "To get started, clone this repo and set up new environment in python 3.10. Then, install the libraries listed in requirements. txt using\n",
    "pip."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dc39822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:39.500446Z",
     "start_time": "2025-09-17T06:48:39.470849Z"
    }
   },
   "source": [
    "# ! conda create -n new_env python=3.10.18\n",
    "# ! conda activate new_env\n",
    "# ! pip install -f requirements.txt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "54938432",
   "metadata": {},
   "source": [
    "If you generating audio data in bulk, please note the following folders to park your raw data:\n",
    "|Folder|Description|\n",
    "|:---|:---|\n",
    "|\"./data/00_raw_speech/\"| Folder for clean speech data |\n",
    "|\"./data/01_stationary_noise/\" | Folder for stationary noise (e.g. background buzz, human chatter in crowded environment, etc.) |\n",
    "|\"./data/02_non-stationary_noise/\" | Folder for non-stationary noise (e.g. Pen dropping, bell-ringing, passing car, etc.) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d10d67",
   "metadata": {},
   "source": [
    "### III. Synthesis Walkthrough\n",
    "This walkthrough will provide an overview of the speech synthesis components in FAST."
   ]
  },
  {
   "cell_type": "code",
   "id": "5843321f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:41.881229Z",
     "start_time": "2025-09-17T06:48:39.504282Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf # to export librosa arrays into wav\n",
    "import tempfile\n",
    "from torchaudio import transforms\n",
    "# Helper functions to plot graph, spectrum, load audio\n",
    "from src.helper_functions import plot_waveform, plot_spectrogram, load_audio_with_pytorch\n",
    "# Function to convolve audio with IRs\n",
    "from src.ir_convolve import ir_convolve\n",
    "# Function to right size audio data after convolution\n",
    "from src.post_convo_sizer import post_convo_sizer\n",
    "# Function to add effects to audio\n",
    "from src.audio_effects_new import audio_effector\n",
    "# Function to build noises from repo\n",
    "from src.noise_builder import noise_builder\n",
    "# Function to attach noise to speech data\n",
    "from src.audio_stacker import audio_noise_stack\n",
    "# Function to perform opus encoding decoding\n",
    "from src.encoding_scripts.opus import encode_opus, decode_opus\n",
    "# Function to perform bulk audio generation\n",
    "from src.bulk_generation import bulk_generation\n",
    "# Function to regenerate dataset\n",
    "from src.regenerate_dataset import regenerate_dataset\n",
    "\n",
    "# Confirm torch is working\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n",
      "2.7.0+cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a8477da6",
   "metadata": {},
   "source": [
    "#### III-A: Load Audio and Apply Tempo / Pitch Shift Effects (Obtain new clean audio)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a3e295e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:41.983237Z",
     "start_time": "2025-09-17T06:48:41.947716Z"
    }
   },
   "source": [
    "## Load an audio using a load_audio_with_pytorch\n",
    "sample_audio = \"./data/Samples/raw_clips/Gump.wav\"\n",
    "sample_data, sr = load_audio_with_pytorch(sample_audio)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio './data/Samples/raw_clips/Gump.wav' loaded! Native Sampling Rate: 44100Hz; Shape: torch.Size([1, 351983])\n",
      "Resampling audio from 44100Hz to 16000Hz...\n",
      "Audio resampled. New shape: torch.Size([1, 127704])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b1c69c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:41.998945Z",
     "start_time": "2025-09-17T06:48:41.987989Z"
    }
   },
   "source": [
    "print(sample_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127704])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d3d15e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.223204Z",
     "start_time": "2025-09-17T06:48:42.013629Z"
    }
   },
   "source": [
    "## Implement tempo and pitch shift effects on speech data\n",
    "## Post-implementation, this will be your \"clean\" data for the clean-dirty pair\n",
    "sample_data, sr, _ = audio_effector (sample_data,\n",
    "                                     tempo_change=True,\n",
    "                                     pitch_shift=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "0592fab2",
   "metadata": {},
   "source": [
    "#### III-B: Synthesising Speech with Room Reverberation"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fa23b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.244036Z",
     "start_time": "2025-09-17T06:48:42.231472Z"
    }
   },
   "source": [
    "print(sample_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 123541])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "0dfbc04a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.262639Z",
     "start_time": "2025-09-17T06:48:42.246837Z"
    }
   },
   "source": [
    "## Convolve audio_data with Specific Room Impusle Response fi l e\n",
    "rir_path = \"./data/Impulse_Responses/room_IRs/Room007-00007.wav.npy\"\n",
    "sample_data, sr, size_orig, IR_applied, _ = ir_convolve(sample_data, sr, mode=\"specific\", specific_ir_path=rir_path)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ea359aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.273788Z",
     "start_time": "2025-09-17T06:48:42.268414Z"
    }
   },
   "source": [
    "print(sample_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 131540])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "fcad3ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.302065Z",
     "start_time": "2025-09-17T06:48:42.281959Z"
    }
   },
   "source": [
    "# Right size convolved audio\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"room\",\n",
    "                               IR_applied=IR_applied)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR peak detected at sample #134\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "46dd542c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.317706Z",
     "start_time": "2025-09-17T06:48:42.308285Z"
    }
   },
   "source": [
    "print(sample_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 123541])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "94dcc4ae",
   "metadata": {},
   "source": [
    "#### III-C: Synthesising Noise"
   ]
  },
  {
   "cell_type": "code",
   "id": "c06db4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.369144Z",
     "start_time": "2025-09-17T06:48:42.332737Z"
    }
   },
   "source": [
    "## Generate stationary noise and add effects\n",
    "noise_stationary_repo = \"./data/01_stationary_noise/\"\n",
    "noise_stationary_data, sr , _= noise_builder (sample_data,\n",
    "                                           noise_stationary_repo,\n",
    "                                           echo = True,\n",
    "                                           low_pass = True,\n",
    "                                           mode = \"stationary\")\n",
    "\n",
    "## Generate non-stationary noise and convolve with room impulse response\n",
    "noise_nonstationary_repo = \"./data/02_non-stationary_noise/\"\n",
    "noise_nonstationary_data, sr, _ = noise_builder (sample_data,\n",
    "                                              noise_nonstationary_repo,\n",
    "                                              echo = True,\n",
    "                                              mode = \"non-stationary\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio './data/01_stationary_noise/noise-free-sound-0001.wav' loaded! Native Sampling Rate: 16000Hz; Shape: torch.Size([1, 648584])\n",
      "Audio './data/02_non-stationary_noise/noise-free-sound-0069.wav' loaded! Native Sampling Rate: 16000Hz; Shape: torch.Size([1, 41143])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "923e76ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.401350Z",
     "start_time": "2025-09-17T06:48:42.381853Z"
    }
   },
   "source": [
    "print(noise_nonstationary_data.shape)\n",
    "print(noise_nonstationary_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 123541])\n",
      "torch.Size([1, 123541])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "82e37119",
   "metadata": {},
   "source": [
    "#### III-D: Combining Speech and Noise"
   ]
  },
  {
   "cell_type": "code",
   "id": "753c16fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.417370Z",
     "start_time": "2025-09-17T06:48:42.407330Z"
    }
   },
   "source": [
    "## Stack Noises-s onto each other, then stack combined noise onto speech data\n",
    "# Stack stationary and non-stationary noise first\n",
    "combined_noise_data = audio_noise_stack(noise_stationary_data, noise_nonstationary_data, SNR=3)\n",
    "\n",
    "# Stack speech onto noise\n",
    "sample_data = audio_noise_stack(sample_data, combined_noise_data, SNR=0)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "df165418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.433391Z",
     "start_time": "2025-09-17T06:48:42.424390Z"
    }
   },
   "source": [
    "print(sample_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 123541])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "2d695f75",
   "metadata": {},
   "source": [
    "#### III-E: Simulating Passing of Audio through Fabric"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2432758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.458959Z",
     "start_time": "2025-09-17T06:48:42.442853Z"
    }
   },
   "source": [
    "## Convolve data with Fabric IR\n",
    "fabric_ir_path = \"./data/Impulse_Responses/fabric_IRs/ir_9LC_0deg_aligned_smooth.npy\"\n",
    "sample_data, sr, size_orig, IR_applied, _ = ir_convolve(sample_data, \n",
    "                                                    sr,\n",
    "                                                    mode=\"specific\",\n",
    "                                                    specific_ir_path=fabric_ir_path)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "23d60574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.473602Z",
     "start_time": "2025-09-17T06:48:42.458959Z"
    }
   },
   "source": [
    "sample_data.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124140])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "422bd23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.491077Z",
     "start_time": "2025-09-17T06:48:42.475750Z"
    }
   },
   "source": [
    "## Right size convolved data\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"fabric\",\n",
    "                               IR_applied=IR_applied)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "d880ba01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.513526Z",
     "start_time": "2025-09-17T06:48:42.491504Z"
    }
   },
   "source": [
    "sample_data.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 123541])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "97c8584f",
   "metadata": {},
   "source": [
    "#### III-F: Simulating Recording of Audio by Mobile Phones"
   ]
  },
  {
   "cell_type": "code",
   "id": "da66d406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.543087Z",
     "start_time": "2025-09-17T06:48:42.517205Z"
    }
   },
   "source": [
    "## Convolved data with Mobile Phone IR\n",
    "phone_ir_path = \"./data/Impulse_Responses/handphone_IRs/IR_rog_phone_3.npy\"\n",
    "sample_data, sr, size_orig, chosen_ir, _ = ir_convolve(sample_data, \n",
    "                                                    sr,\n",
    "                                                    mode=\"specific\",\n",
    "                                                    specific_ir_path=phone_ir_path)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "5c102642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.558248Z",
     "start_time": "2025-09-17T06:48:42.543087Z"
    }
   },
   "source": [
    "sample_data.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 126540])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "a4f01219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.578075Z",
     "start_time": "2025-09-17T06:48:42.558248Z"
    }
   },
   "source": [
    "## Right size convolved data\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"mobile\",\n",
    "                               IR_applied=IR_applied)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR peak detected at sample #0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "c76a7d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:42.603083Z",
     "start_time": "2025-09-17T06:48:42.586396Z"
    }
   },
   "source": [
    "sample_data.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 123541])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "9213ef38",
   "metadata": {},
   "source": [
    "#### III-G. Simulating Degradation of Audio from Mobile CODEC Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "369ec517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T06:48:43.143528Z",
     "start_time": "2025-09-17T06:48:42.610701Z"
    }
   },
   "source": [
    "## Export audio into temp folder\n",
    "# Create temp_folder to house current state to sample_data\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    temp_file_path = os.path.join(tmpdirname, \"sample_audio.wav\")\n",
    "    torchaudio.save(temp_file_path, sample_data, sample_rate=sr, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "\n",
    "    ## Encode and Decode audio\n",
    "    opus_encoded_path = encode_opus(wav_path = temp_file_path,\n",
    "                                    tmp_folder=tmpdirname)\n",
    "    opus_decoded_path = decode_opus(opus_encoded_path=opus_encoded_path,\n",
    "                                    output_folder=\"./output/sample_outputs\")"
   ],
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'echo y | ffmpeg -i C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_ug19vs\\sample_audio.opus -c:a pcm_s16le -ar 16000 -nostats -hide_banner -loglevel error ./output/sample_outputs\\_C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_ug19vs\\sample_audio_opus_decoded.wav' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m## Encode and Decode audio\u001B[39;00m\n\u001B[0;32m      9\u001B[0m opus_encoded_path \u001B[38;5;241m=\u001B[39m encode_opus(wav_path \u001B[38;5;241m=\u001B[39m temp_file_path,\n\u001B[0;32m     10\u001B[0m                                 tmp_folder\u001B[38;5;241m=\u001B[39mtmpdirname)\n\u001B[1;32m---> 11\u001B[0m opus_decoded_path \u001B[38;5;241m=\u001B[39m \u001B[43mdecode_opus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopus_encoded_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopus_encoded_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                \u001B[49m\u001B[43moutput_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./output/sample_outputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\farfield-audio-synthesis-toolkit\\src\\encoding_scripts\\opus.py:23\u001B[0m, in \u001B[0;36mdecode_opus\u001B[1;34m(opus_encoded_path, output_folder, encoding, sampling_rate, count, decoded_path)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# control encoding \"-c:a pcm_s16le\"; sampling_rate with \"-ar 48000\"\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# echo y to overwrite by default\u001B[39;00m\n\u001B[0;32m     22\u001B[0m command \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mecho y | ffmpeg -i \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopus_encoded_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -c:a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mencoding\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -ar \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msampling_rate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -nostats -hide_banner -loglevel error \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdecoded_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 23\u001B[0m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m decoded_path\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\farfield-audio-synthesis-toolkit\\lib\\subprocess.py:526\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    524\u001B[0m     retcode \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mpoll()\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m check \u001B[38;5;129;01mand\u001B[39;00m retcode:\n\u001B[1;32m--> 526\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(retcode, process\u001B[38;5;241m.\u001B[39margs,\n\u001B[0;32m    527\u001B[0m                                  output\u001B[38;5;241m=\u001B[39mstdout, stderr\u001B[38;5;241m=\u001B[39mstderr)\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CompletedProcess(process\u001B[38;5;241m.\u001B[39margs, retcode, stdout, stderr)\n",
      "\u001B[1;31mCalledProcessError\u001B[0m: Command 'echo y | ffmpeg -i C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_ug19vs\\sample_audio.opus -c:a pcm_s16le -ar 16000 -nostats -hide_banner -loglevel error ./output/sample_outputs\\_C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_ug19vs\\sample_audio_opus_decoded.wav' returned non-zero exit status 1."
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "0ea0d5b0",
   "metadata": {},
   "source": [
    "### IV. Bulk Audio Generation\n",
    "\n",
    "What you have above will suffice to prepare a noisy, reveberant clip. For large scale training, you can generate bulk audio using the\n",
    "function below.\n",
    "\n",
    "This function randomly draws upon the data stored your data sub-folders (check documentation for default randomising parameters)\n",
    "to synthesize a large audio dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "f74289c9",
   "metadata": {},
   "source": [
    "bulk_generation(number_of_audios = 5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96631b74",
   "metadata": {},
   "source": [
    "### V. Experimental Results (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3672d0",
   "metadata": {},
   "source": [
    "We are still in the process of using the audio synthesis by FAST to train denosing/STT models! We hope to update this soon - with\n",
    "good news nevertheless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056045",
   "metadata": {},
   "source": [
    "### VI. Regeneration of Datasets\n",
    "\n",
    "Using bulk generation will produce a json file in the output folder. This json file logs the parameter used to produce a dataset, which can be used to reproduce a dataset that was previously generated using the function below.\n",
    "\n",
    "Note that this requires the original speech files, noise files, IR files, and codec to be inside their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "id": "cc86b96a",
   "metadata": {},
   "source": [
    "# Function to regenerate dataset\n",
    "from src.regenerate_dataset import regenerate_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30186cbd",
   "metadata": {},
   "source": [
    "## Reproduce dataset\n",
    "test = regenerate_dataset(log_json=\"./output/experiment_log.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0eebca7",
   "metadata": {},
   "source": [
    "### VII. Contact Us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f5b3d",
   "metadata": {},
   "source": [
    "Have a feedback? Have a Question? Wish to Collaborate?\n",
    "Perhaps one of our future efforts excites you?\n",
    "Feel free to reach out to our Speech Researchers @ angjunsiong@gmail.com:\n",
    "\n",
    "- Simon Chee (Boss)\n",
    "- Avery Khoo (Tech Lead)\n",
    "- Jun Siong\n",
    "- Rebecca Oel\n",
    "- Winfred Kong\n",
    "\n",
    "Future Efforts:\n",
    "- (i) Custom RIR: Integrate libraries like pyroomacoustics, soundscapes to generate bespoke Room IRs\n",
    "- (ii) Evaluation of Denoising / STT Models trained with FAST data: Evaluate utility of synthesised audio in improving Denoising/\n",
    "STT models\n",
    "- (ii) Generate Overlapping Speakers / Time-Stamp Labelling Systems: To expand use case to speaker separation and VAD\n",
    "models training\n",
    "- (iv) Ablation Studies / FAST Economisation: Investigate relative importance of each simulation steps and strealime/improve model\n",
    "where applicable\n",
    "- (v) Harvest Real Noise Data: Put real data through VAD and extract noise component to enrich noise database\n",
    "- (vi) Create output log for reproducibility of data set: Function to replicate dataset from generation log\n",
    "- (vii) Streaming input: Reduce storage space by synthesising audio on the fly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
