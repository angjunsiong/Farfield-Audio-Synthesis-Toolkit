{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a30ce8",
   "metadata": {},
   "source": [
    "## Far-Field Audio Synthesis Toolkit (FAST) v0.2: User Guide\n",
    "This notebook serves as a user guide for the FAST toolkit, designed to rapidly synthesis audio to train/fine-tune audio ML models.\n",
    "\n",
    "#### ***v0.2 enhancements\n",
    "- Developed generation parameters logging and data regeneration function\n",
    "- Fixed bugs causing errors for some generation scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964f44",
   "metadata": {},
   "source": [
    "### Table of Content:\n",
    "- I. [Overview](#i-overview)\n",
    "- II. [Getting Started](#ii-overview)\n",
    "- III. [Synthesis Walkthrough](#iii-synthesis-walkthrough)\n",
    "- IV. [Bulk Audio Generation](#iv-bulk-audio-generation)\n",
    "- V. [Experimental Results (WIP)](#v-experimental-results-wip)\n",
    "- VI. [Regenerate Dataset](#vi-regeneration-of-datasets)\n",
    "- VII. [Contact Us](#vi-contact-us)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aa10b",
   "metadata": {},
   "source": [
    "### I. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa35987",
   "metadata": {},
   "source": [
    "#### I-A: Why FAST?\n",
    "- To augment training data outside datasets used for conventional commerical/open-research\n",
    "- Particularly, shortage of noisy labelled noisy data\n",
    "\n",
    "#### II-B: Synthesis Approach\n",
    "- WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f22087",
   "metadata": {},
   "source": [
    "### II. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f73f3",
   "metadata": {},
   "source": [
    "To get started, clone this repo and set up new environment in python 3.10. Then, install the libraries listed in requirements. txt using\n",
    "pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc39822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda create -n new_env python=3.10.18\n",
    "# ! conda activate new_env\n",
    "# ! pip install -f requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54938432",
   "metadata": {},
   "source": [
    "If you generating audio data in bulk, please note the following folders to park your raw data:\n",
    "|Folder|Description|\n",
    "|:---|:---|\n",
    "|\"./data/00_raw_speech/\"| Folder for clean speech data |\n",
    "|\"./data/01_stationary_noise/\" | Folder for stationary noise (e.g. background buzz, human chatter in crowded environment, etc.) |\n",
    "|\"./data/02_non-stationary_noise/\" | Folder for non-stationary noise (e.g. Pen dropping, bell-ringing, passing car, etc.) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d10d67",
   "metadata": {},
   "source": [
    "### III. Synthesis Walkthrough\n",
    "This walkthrough will provide an overview of the speech synthesis components in FAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5843321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf # to export librosa arrays into wav\n",
    "import tempfile\n",
    "from torchaudio import transforms\n",
    "# Helper functions to plot graph, spectrum, load audio\n",
    "from src.helper_functions import plot_waveform, plot_specgram, load_audio_with_pytorch\n",
    "# Function to convolve audio with IRs\n",
    "from src.ir_convolve import ir_convolve\n",
    "# Function to right size audio data after convolution\n",
    "from src.post_convo_sizer import post_convo_sizer\n",
    "# Function to add effects to audio\n",
    "from src.audio_effects_new import audio_effector\n",
    "# Function to build noises from repo\n",
    "from src.noise_builder import noise_builder\n",
    "# Function to attach noise to speech data\n",
    "from src.audio_stacker import audio_noise_stack\n",
    "# Function to perform opus encoding decoding\n",
    "from src.encoding_scripts.opus import encode_opus, decode_opus\n",
    "# Function to perform bulk audio generation\n",
    "from src.bulk_generation import bulk_generation\n",
    "from src.bulk_generation_simple import bulk_generation_simple\n",
    "# Function to regenerate dataset\n",
    "from src.regenerate_dataset import regenerate_dataset\n",
    "\n",
    "# Confirm torch is working\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8477da6",
   "metadata": {},
   "source": [
    "#### III-A: Load Audio and Apply Tempo / Pitch Shift Effects (Obtain new clean audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3e295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/Samples/raw_clips/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/Samples/raw_clips/Gump.wav resampled to 16000Hz\n"
     ]
    }
   ],
   "source": [
    "## Load an audio using a load_audio_with_pytorch\n",
    "sample_audio = \"./data/Samples/raw_clips/Gump.wav\"\n",
    "sample_data, sr = load_audio_with_pytorch(sample_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c69c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127704])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d15e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement tempo and pitch shift effects on speech data\n",
    "## Post-implementation, this will be your \"clean\" data for the clean-dirty pair\n",
    "sample_data, sr, _ = audio_effector (sample_data,\n",
    "                                     tempo_change=True,\n",
    "                                     pitch_shift=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592fab2",
   "metadata": {},
   "source": [
    "#### III-B: Synthesising Speech with Room Reverberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa23b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 117047])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfbc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolve audio_data with Specific Room Impusle Response fi l e\n",
    "rir_path = \"./data/Impulse_Responses/room_IRs/Room007-00007.wav.npy\"\n",
    "sample_data, sr, size_orig, IR_applied, _ = ir_convolve(sample_data, sr, mode=\"specific\", specific_ir_path=rir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea359aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 125046])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcad3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR peak detected at sample #134\n"
     ]
    }
   ],
   "source": [
    "# Right size convolved audio\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"room\",\n",
    "                               IR_applied=IR_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46dd542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 117047])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcc4ae",
   "metadata": {},
   "source": [
    "#### III-C: Synthesising Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06db4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/01_stationary_noise/noise-free-sound-0064.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1602133, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n"
     ]
    }
   ],
   "source": [
    "## Generate stationary noise and add effects\n",
    "noise_stationary_repo = \"./data/01_stationary_noise/\"\n",
    "noise_stationary_data, sr , _= noise_builder (sample_data,\n",
    "                                           noise_stationary_repo,\n",
    "                                           echo = True,\n",
    "                                           low_pass = True,\n",
    "                                           mode = \"stationary\")\n",
    "\n",
    "## Generate non-stationary noise and convolve with room impulse response\n",
    "noise_nonstationary_repo = \"./data/02_non-stationary_noise/\"\n",
    "noise_nonstationary_data, sr, _ = noise_builder (sample_data,\n",
    "                                              noise_nonstationary_repo,\n",
    "                                              echo = True,\n",
    "                                              mode = \"non-stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923e76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 117047])\n",
      "torch.Size([1, 117047])\n"
     ]
    }
   ],
   "source": [
    "print(noise_nonstationary_data.shape)\n",
    "print(noise_nonstationary_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e37119",
   "metadata": {},
   "source": [
    "#### III-D: Combining Speech and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753c16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stack Noises-s onto each other, then stack combined noise onto speech data\n",
    "# Stack stationary and non-stationary noise first\n",
    "combined_noise_data = audio_noise_stack(noise_stationary_data, noise_nonstationary_data, SNR=3)\n",
    "\n",
    "# Stack speech onto noise\n",
    "sample_data = audio_noise_stack(sample_data, combined_noise_data, SNR=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df165418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 117047])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695f75",
   "metadata": {},
   "source": [
    "#### III-E: Simulating Passing of Audio through Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2432758",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolve data with Fabric IR\n",
    "fabric_ir_path = \"./data/Impulse_Responses/fabric_IRs/ir_9LC_0deg_aligned_smooth.npy\"\n",
    "sample_data, sr, size_orig, IR_applied, _ = ir_convolve(sample_data, \n",
    "                                                    sr,\n",
    "                                                    mode=\"specific\",\n",
    "                                                    specific_ir_path=fabric_ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d60574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 117646])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422bd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Right size convolved data\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"fabric\",\n",
    "                               IR_applied=IR_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d880ba01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 117047])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8584f",
   "metadata": {},
   "source": [
    "#### III-F: Simulating Recording of Audio by Mobile Phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da66d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolved data with Mobile Phone IR\n",
    "phone_ir_path = \"./data/Impulse_Responses/handphone_IRs/IR_rog_phone_3.npy\"\n",
    "sample_data, sr, size_orig, chosen_ir, _ = ir_convolve(sample_data, \n",
    "                                                    sr,\n",
    "                                                    mode=\"specific\",\n",
    "                                                    specific_ir_path=phone_ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c102642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120046])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4f01219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR peak detected at sample #0\n"
     ]
    }
   ],
   "source": [
    "## Right size convolved data\n",
    "sample_data = post_convo_sizer(audio_data=sample_data, \n",
    "                               size_orig = size_orig, \n",
    "                               convo_type=\"mobile\",\n",
    "                               IR_applied=IR_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c76a7d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 117047])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213ef38",
   "metadata": {},
   "source": [
    "#### III-G. Simulating Degradation of Audio from Mobile CODEC Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369ec517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/sample_outputs/_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    }
   ],
   "source": [
    "## Export audio into temp folder\n",
    "# Create temp_folder to house current state to sample_data\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    temp_file_path = os.path.join(tmpdirname, \"sample_audio.wav\")\n",
    "    torchaudio.save(temp_file_path, sample_data, sample_rate=sr, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "\n",
    "    ## Encode and Decode audio\n",
    "    opus_encoded_path = encode_opus(wav_path = temp_file_path,\n",
    "                                    tmp_folder=tmpdirname)\n",
    "    opus_decoded_path = decode_opus(opus_encoded_path=opus_encoded_path,\n",
    "                                    output_folder=\"./output/sample_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0d5b0",
   "metadata": {},
   "source": [
    "### IV. Bulk Audio Generation\n",
    "\n",
    "What you have above will suffice to prepare a noisy, reveberant clip. For large scale training, you can generate bulk audio using the\n",
    "function below.\n",
    "\n",
    "This function randomly draws upon the data stored your data sub-folders (check documentation for default randomising parameters)\n",
    "to synthesize a large audio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87932b82",
   "metadata": {},
   "source": [
    "#### IV-A. Bulk Generation with Fabric IR, Room IR, and Mobile Codec Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74289c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #56\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0021.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([288725, 1])\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0004.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([47784, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0025.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([37547, 1])\n",
      "IR peak detected at sample #1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/dirty_samples/0_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio ./output/dirty_samples/0_sample_audio_opus_decoded.wav generated!\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #56\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0030.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1168196, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "IR peak detected at sample #1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/dirty_samples/1_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio ./output/dirty_samples/1_sample_audio_opus_decoded.wav generated!\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #88\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0030.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1168196, 1])\n",
      "IR peak detected at sample #1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/dirty_samples/2_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio ./output/dirty_samples/2_sample_audio_opus_decoded.wav generated!\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #187\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0064.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1602133, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "IR peak detected at sample #1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/dirty_samples/3_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio ./output/dirty_samples/3_sample_audio_opus_decoded.wav generated!\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #147\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0021.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([288725, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "IR peak detected at sample #1421\n",
      "audio ./output/dirty_samples/4_sample_audio_opus_decoded.wav generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/dirty_samples/4_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    }
   ],
   "source": [
    "bulk_generation(number_of_audios = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00731a44",
   "metadata": {},
   "source": [
    "#### IV-B. Bulk Generation with Simple Low/Band-Pass Filters in Place of Fabric IR, Mobile IR, and Mobile Codec Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a8204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #134\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0030.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1168196, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "torch.Size([1, 135263])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 135263])\n",
      "<class 'torch.Tensor'>\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #187\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0001.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([648584, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "torch.Size([1, 142024])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 142024])\n",
      "<class 'torch.Tensor'>\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #213\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0001.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([648584, 1])\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0021.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([288725, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "torch.Size([1, 152783])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 152783])\n",
      "<class 'torch.Tensor'>\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #148\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0001.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([648584, 1])\n",
      "torch.Size([1, 114115])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 114115])\n",
      "<class 'torch.Tensor'>\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #56\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0001.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([648584, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0004.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([47784, 1])\n",
      "torch.Size([1, 126025])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 126025])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "bulk_generation_simple(number_of_audios = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96631b74",
   "metadata": {},
   "source": [
    "### V. Experimental Results (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3672d0",
   "metadata": {},
   "source": [
    "We are still in the process of using the audio synthesis by FAST to train denosing/STT models! We hope to update this soon - with\n",
    "good news nevertheless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc056045",
   "metadata": {},
   "source": [
    "### VI. Regeneration of Datasets\n",
    "\n",
    "Using bulk generation will produce a json file in the output folder. This json file logs the parameter used to produce a dataset, which can be used to reproduce a dataset that was previously generated using the function below.\n",
    "\n",
    "Note that this requires the original speech files, noise files, IR files, and codec to be inside their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc86b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to regenerate dataset\n",
    "from src.regenerate_dataset import regenerate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30186cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 audio files to be regenerated...\n",
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #56\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0021.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([288725, 1])\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0004.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([47784, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0025.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([37547, 1])\n",
      "IR peak detected at sample #1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/regenerated_samples/0_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #56\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0030.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1168196, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0060.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([16718, 1])\n",
      "IR peak detected at sample #1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/regenerated_samples/1_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #88\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0030.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1168196, 1])\n",
      "IR peak detected at sample #1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/regenerated_samples/2_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #187\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0064.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([1602133, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "IR peak detected at sample #1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/regenerated_samples/3_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio ./data/00_raw_speech/Gump.wav loaded!; Native Sampling_Rate: 44100Hz; Shape: torch.Size([351983, 1])\n",
      "Audio ./data/00_raw_speech/Gump.wav resampled to 16000Hz\n",
      "IR peak detected at sample #147\n",
      "Audio ./data/01_stationary_noise/noise-free-sound-0021.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([288725, 1])\n",
      "Audio ./data/02_non-stationary_noise/noise-free-sound-0069.wav loaded!; Native Sampling_Rate: 16000Hz; Shape: torch.Size([41143, 1])\n",
      "IR peak detected at sample #1421\n",
      "Regeneration Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File './output/regenerated_samples/4_sample_audio_opus_decoded.wav' already exists. Overwrite? [y/N] "
     ]
    }
   ],
   "source": [
    "## Reproduce dataset\n",
    "test = regenerate_dataset(log_json=\"./output/experiment_log_250926_010900.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eebca7",
   "metadata": {},
   "source": [
    "### VII. Contact Us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f5b3d",
   "metadata": {},
   "source": [
    "Have a feedback? Have a Question? Wish to Collaborate?\n",
    "Perhaps one of our future efforts excites you?\n",
    "Feel free to reach out to our Speech Researchers @ angjunsiong@gmail.com:\n",
    "\n",
    "- Simon Chee (Boss)\n",
    "- Avery Khoo (Tech Lead)\n",
    "- Jun Siong\n",
    "- Rebecca Oel\n",
    "- Winfred Kong\n",
    "\n",
    "Future Efforts:\n",
    "- (i) Custom RIR: Integrate libraries like pyroomacoustics, soundscapes to generate bespoke Room IRs\n",
    "- (ii) Evaluation of Denoising / STT Models trained with FAST data: Evaluate utility of synthesised audio in improving Denoising/\n",
    "STT models\n",
    "- (ii) Generate Overlapping Speakers / Time-Stamp Labelling Systems: To expand use case to speaker separation and VAD\n",
    "models training\n",
    "- (iv) Ablation Studies / FAST Economisation: Investigate relative importance of each simulation steps and strealime/improve model\n",
    "where applicable\n",
    "- (v) Harvest Real Noise Data: Put real data through VAD and extract noise component to enrich noise database\n",
    "- (vi) Create output log for reproducibility of data set: Function to replicate dataset from generation log\n",
    "- (vii) Streaming input: Reduce storage space by synthesising audio on the fly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
